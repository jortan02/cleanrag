import traceback
from typing import List, Dict, Any, Optional
import pandas as pd
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,
    context_precision,
    answer_correctness,
)
import ast # For safely evaluating string representations of lists from CSV

# Use TYPE_CHECKING for SessionManager to avoid circular imports
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from .session_manager import SessionManager
    from llama_index.core.indices.base import BaseIndex
    from llama_index.core.query_engine import BaseQueryEngine

# Define consistent column names to be used
COL_QUESTION = "question"
COL_ANSWER_GROUND_TRUTH = "answer" # Column name in user's CSV for the ground truth answer
COL_CONTEXTS_GROUND_TRUTH = "ground_truth_contexts" # Optional column in user's CSV

# Column names for data generated by the RAG pipeline
COL_ANSWER_GENERATED = "generated_answer"
COL_CONTEXTS_RETRIEVED = "retrieved_contexts"


def _parse_contexts_from_csv_cell(cell_value: Any) -> List[str]:
    """
    Safely parses a CSV cell value that might contain a list of context strings.
    Handles strings like "['context1', 'context2']" or just a single string.
    """
    if pd.isna(cell_value):
        return []
    if isinstance(cell_value, list): # Already a list (e.g., if DataFrame constructed programmatically)
        return [str(item) for item in cell_value]
    if isinstance(cell_value, str):
        if cell_value.startswith('[') and cell_value.endswith(']'):
            try:
                # Safely evaluate string representation of a list
                parsed_list = ast.literal_eval(cell_value)
                if isinstance(parsed_list, list):
                    return [str(item) for item in parsed_list] # Ensure all items are strings
                else: # Parsed but not a list, treat as single item
                    return [str(parsed_list)]
            except (ValueError, SyntaxError):
                # Not a valid list string, treat as a single context string
                return [cell_value]
        else:
            # Just a plain string, treat as a single context
            return [cell_value]
    # For other types, convert to string and treat as single context
    return [str(cell_value)]


def generate_rag_outputs_for_evaluation(sm: 'SessionManager') -> List[Dict[str, Any]]:
    """
    Runs questions from the QA set through the LlamaIndex RAG pipeline
    to get generated answers and retrieved contexts. Also includes ground truth data.
    """
    qa_data_df: pd.DataFrame | None = sm.get_qa_data()
    index: 'BaseIndex' | None = sm.index

    if qa_data_df is None or qa_data_df.empty:
        print("QA data is empty. Cannot generate RAG outputs for evaluation.")
        return []
    if index is None:
        print("LlamaIndex Index not found. Cannot generate RAG outputs for evaluation.")
        return []

    query_engine: 'BaseQueryEngine' = index.as_query_engine()
    
    rag_outputs = []
    # Check if the optional ground truth contexts column exists
    has_gt_contexts_column = COL_CONTEXTS_GROUND_TRUTH in qa_data_df.columns

    for idx, row in qa_data_df.iterrows():
        question = str(row[COL_QUESTION])
        ground_truth_answer = str(row[COL_ANSWER_GROUND_TRUTH])
        
        user_provided_gt_contexts = []
        if has_gt_contexts_column:
            user_provided_gt_contexts = _parse_contexts_from_csv_cell(row[COL_CONTEXTS_GROUND_TRUTH])

        output_item = {
            COL_QUESTION: question,
            COL_ANSWER_GROUND_TRUTH: ground_truth_answer,
        }
        if has_gt_contexts_column: # Only include if column was present in input
             output_item[COL_CONTEXTS_GROUND_TRUTH] = user_provided_gt_contexts


        try:
            response = query_engine.query(question)
            output_item[COL_ANSWER_GENERATED] = str(response.response)
            output_item[COL_CONTEXTS_RETRIEVED] = [
                node.get_content() for node in response.source_nodes
            ] if response.source_nodes else []
            
        except Exception as e:
            print(f"Error querying LlamaIndex for question '{question}': {e}")
            output_item[COL_ANSWER_GENERATED] = f"Error during RAG generation: {e}"
            output_item[COL_CONTEXTS_RETRIEVED] = []
        
        rag_outputs.append(output_item)
            
    return rag_outputs


def run_ragas_evaluation_core( # Renamed to indicate it's the core Ragas step
    qa_data_df_for_ragas: pd.DataFrame, # Expects DataFrame with Ragas columns
    metrics_to_run: List[Any]
) -> pd.DataFrame:
    """
    Core Ragas evaluation logic.
    Args:
        qa_data_df_for_ragas: DataFrame formatted for Ragas (question, answer, contexts, ground_truth, optional ground_truth_contexts).
        metrics_to_run: List of Ragas metric functions.
    Returns:
        pd.DataFrame: The Ragas results.
    """
    hf_dataset = Dataset.from_pandas(qa_data_df_for_ragas)
    print(f"Running Ragas evaluation with metrics: {[m.name for m in metrics_to_run]}")
    try:
        result = evaluate(
            dataset=hf_dataset,
            metrics=metrics_to_run,
        )
        results_df = result.to_pandas()
        results_df["retrieved_contexts"] = results_df["retrieved_contexts"].apply(lambda x: "\n--- END OF CONTEXT ---\n".join(x))
        print("Ragas evaluation core step completed.")
        return results_df
    except Exception as e:
        print(f"Error during Ragas core evaluation: {e}")
        # Return a DataFrame with an error column for easier debugging
        error_df = qa_data_df_for_ragas.copy() # Show what data was passed
        error_df['ragas_evaluation_error'] = str(e)
        error_df['ragas_evaluation_traceback'] = traceback.format_exc()
        return error_df

def prepare_and_run_ragas_evaluation(sm: 'SessionManager') -> Optional[pd.DataFrame]:
    """
    Prepares data by generating RAG outputs and then runs Ragas evaluation.
    Returns the Ragas results DataFrame or None if preparation fails.
    """
    print("Preparing RAG outputs for evaluation...")
    rag_pipeline_outputs = generate_rag_outputs_for_evaluation(sm) # List[Dict]

    if not rag_pipeline_outputs:
        return None

    # Prepare DataFrame for Ragas
    dataset_for_ragas_list = []
    has_ground_truth_contexts_data = False
    if rag_pipeline_outputs and COL_CONTEXTS_GROUND_TRUTH in rag_pipeline_outputs[0]:
        has_ground_truth_contexts_data = True

    for item in rag_pipeline_outputs:
        row = {
            "question": item[COL_QUESTION],
            "answer": item[COL_ANSWER_GENERATED], # RAG's answer
            "contexts": item[COL_CONTEXTS_RETRIEVED], # RAG's contexts
            "ground_truth": item[COL_ANSWER_GROUND_TRUTH] # User's GT answer
        }
        if has_ground_truth_contexts_data:
            row["ground_truth_contexts"] = item.get(COL_CONTEXTS_GROUND_TRUTH, [])
        dataset_for_ragas_list.append(row)
    
    qa_data_for_ragas_df = pd.DataFrame(dataset_for_ragas_list)

    # Define Ragas metrics
    metrics_to_run = [
        faithfulness, answer_relevancy, answer_correctness, context_precision,
    ]
    if has_ground_truth_contexts_data:
        metrics_to_run.append(context_recall)
        print(f"'{COL_CONTEXTS_GROUND_TRUTH}' column found. Adding 'context_recall' to Ragas metrics.")
    else:
        print(f"'{COL_CONTEXTS_GROUND_TRUTH}' column not found. 'Context Recall' specific to GT contexts will not be run (Ragas might use a fallback).")

    return run_ragas_evaluation_core(qa_data_for_ragas_df, metrics_to_run)