from typing import List, Dict, Any, Optional
import pandas as pd
from datasets import Dataset # Ragas uses Hugging Face Datasets
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_recall,      # Will be conditional
    context_precision,
    answer_correctness,  # Assumes 'answer' in CSV is ground_truth_answer
    # answer_similarity # Could be added as another option
)
import ast # For safely evaluating string representations of lists from CSV

# Use TYPE_CHECKING for SessionManager to avoid circular imports
from typing import TYPE_CHECKING
if TYPE_CHECKING:
    from .session_manager import SessionManager
    from llama_index.core.indices.base import BaseIndex
    from llama_index.core.query_engine import BaseQueryEngine

# Define consistent column names to be used
COL_QUESTION = "question"
COL_ANSWER_GROUND_TRUTH = "answer" # Column name in user's CSV for the ground truth answer
COL_CONTEXTS_GROUND_TRUTH = "ground_truth_contexts" # Optional column in user's CSV

# Column names for data generated by the RAG pipeline
COL_ANSWER_GENERATED = "generated_answer"
COL_CONTEXTS_RETRIEVED = "retrieved_contexts"


def _parse_contexts_from_csv_cell(cell_value: Any) -> List[str]:
    """
    Safely parses a CSV cell value that might contain a list of context strings.
    Handles strings like "['context1', 'context2']" or just a single string.
    """
    if pd.isna(cell_value):
        return []
    if isinstance(cell_value, list): # Already a list (e.g., if DataFrame constructed programmatically)
        return [str(item) for item in cell_value]
    if isinstance(cell_value, str):
        if cell_value.startswith('[') and cell_value.endswith(']'):
            try:
                # Safely evaluate string representation of a list
                parsed_list = ast.literal_eval(cell_value)
                if isinstance(parsed_list, list):
                    return [str(item) for item in parsed_list] # Ensure all items are strings
                else: # Parsed but not a list, treat as single item
                    return [str(parsed_list)]
            except (ValueError, SyntaxError):
                # Not a valid list string, treat as a single context string
                return [cell_value]
        else:
            # Just a plain string, treat as a single context
            return [cell_value]
    # For other types, convert to string and treat as single context
    return [str(cell_value)]


def generate_rag_outputs_for_evaluation(sm: 'SessionManager') -> List[Dict[str, Any]]:
    """
    Runs questions from the QA set through the LlamaIndex RAG pipeline
    to get generated answers and retrieved contexts. Also includes ground truth data.
    """
    qa_data_df: pd.DataFrame | None = sm.get_qa_data()
    index: 'BaseIndex' | None = sm.index

    if qa_data_df is None or qa_data_df.empty:
        print("QA data is empty. Cannot generate RAG outputs for evaluation.")
        return []
    if index is None:
        print("LlamaIndex Index not found. Cannot generate RAG outputs for evaluation.")
        return []

    query_engine: 'BaseQueryEngine' = index.as_query_engine()
    
    rag_outputs = []
    # Check if the optional ground truth contexts column exists
    has_gt_contexts_column = COL_CONTEXTS_GROUND_TRUTH in qa_data_df.columns

    for idx, row in qa_data_df.iterrows():
        question = str(row[COL_QUESTION])
        ground_truth_answer = str(row[COL_ANSWER_GROUND_TRUTH])
        
        user_provided_gt_contexts = []
        if has_gt_contexts_column:
            user_provided_gt_contexts = _parse_contexts_from_csv_cell(row[COL_CONTEXTS_GROUND_TRUTH])

        output_item = {
            COL_QUESTION: question,
            COL_ANSWER_GROUND_TRUTH: ground_truth_answer,
        }
        if has_gt_contexts_column: # Only include if column was present in input
             output_item[COL_CONTEXTS_GROUND_TRUTH] = user_provided_gt_contexts


        try:
            response = query_engine.query(question)
            output_item[COL_ANSWER_GENERATED] = str(response.response)
            output_item[COL_CONTEXTS_RETRIEVED] = [
                node.get_content() for node in response.source_nodes
            ] if response.source_nodes else []
            
        except Exception as e:
            print(f"Error querying LlamaIndex for question '{question}': {e}")
            output_item[COL_ANSWER_GENERATED] = f"Error during RAG generation: {e}"
            output_item[COL_CONTEXTS_RETRIEVED] = []
        
        rag_outputs.append(output_item)
            
    return rag_outputs


def run_ragas_evaluation(sm: 'SessionManager') -> pd.DataFrame:
    """
    Prepares data and runs Ragas evaluation on the LlamaIndex RAG pipeline's outputs.
    Adapts metrics based on whether ground_truth_contexts are provided.
    """
    print("Step 1: Generating RAG outputs for Ragas evaluation...")
    rag_pipeline_outputs = generate_rag_outputs_for_evaluation(sm)

    if not rag_pipeline_outputs:
        return pd.DataFrame({"message": ["No RAG outputs were generated for evaluation."]})

    # Prepare data for Ragas Dataset format
    # Ragas expects: 'question', 'answer' (generated), 'contexts' (retrieved), 'ground_truth' (GT answer)
    # and optionally 'ground_truth_contexts'
    
    dataset_dict_for_ragas = {
        "question": [item[COL_QUESTION] for item in rag_pipeline_outputs],
        "answer": [item[COL_ANSWER_GENERATED] for item in rag_pipeline_outputs],
        "contexts": [item[COL_CONTEXTS_RETRIEVED] for item in rag_pipeline_outputs],
        "ground_truth": [item[COL_ANSWER_GROUND_TRUTH] for item in rag_pipeline_outputs]
    }
    
    # Check if the ground_truth_contexts key is present in the first item (implies it was processed)
    # This means the column existed in the input QA data.
    has_ground_truth_contexts_data = False
    if rag_pipeline_outputs and COL_CONTEXTS_GROUND_TRUTH in rag_pipeline_outputs[0]:
        has_ground_truth_contexts_data = True
        dataset_dict_for_ragas["ground_truth_contexts"] = [
            item.get(COL_CONTEXTS_GROUND_TRUTH, []) for item in rag_pipeline_outputs
        ]
        print(f"User-provided '{COL_CONTEXTS_GROUND_TRUTH}' will be used for Ragas context_recall.")

    hf_dataset = Dataset.from_dict(dataset_dict_for_ragas)

    # Define Ragas metrics
    metrics_to_evaluate = [
        faithfulness,
        answer_relevancy,
        answer_correctness, # Compares 'answer' (generated) with 'ground_truth' (GT answer)
        context_precision,  # Compares 'contexts' (retrieved) with 'question'
    ]

    if has_ground_truth_contexts_data:
        # context_recall is most meaningful when user provides ideal ground_truth_contexts
        metrics_to_evaluate.append(context_recall)
        print("Adding 'context_recall' to Ragas metrics as ground truth contexts are available.")
    else:
        print(f"'{COL_CONTEXTS_GROUND_TRUTH}' not available or empty. 'Context Recall' metric will not be explicitly added based on this key, Ragas might behave differently for this metric.")
        # Ragas's context_recall might still run but its interpretation changes without this key.
        # Some versions might require it. If errors occur, this is a place to check.

    print("Step 2: Running Ragas evaluation with selected metrics...")
    try:
        result = evaluate(
            dataset=hf_dataset,
            metrics=metrics_to_evaluate,
            # handle_exceptions=False # Set to True to get partial results if some rows fail
        )
        results_df = result.to_pandas()
        print("Ragas evaluation completed.")
        return results_df
    except Exception as e:
        print(f"Error during Ragas evaluation run: {e}")
        error_df = pd.DataFrame(dataset_dict_for_ragas) # Show what data was passed to Ragas
        error_df['ragas_evaluation_error'] = str(e)
        return error_df